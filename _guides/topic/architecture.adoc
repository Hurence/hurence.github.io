---
layout: vision
permalink: /vision/architecture
---
include::../attributes.adoc[]

= {project-name} - Architecture

{project-name} has an `event driven architecture` that handle all the hard boilerplate code to provide a scalable stream processing framework.

== Core Philosophy : Event sourcing

Most of the systems in this data world can be observables through their **events**.
You just have to look at the https://msdn.microsoft.com/en-us/library/dn589792.aspx[event sourcing pattern] to get an idea of how we could define any system state as a sequence of temporal events. The core nature of events can be very different from one organisation to another but consists mainly in `system/application logs`, `sensor data`, `user click streams`, etc. Large and complex systems, made of number of heterogeneous components are not easy to monitor, especially when have to deal with distributed computing. Most of the time of IT resources is spent in maintenance tasks, so there's a real need for tools to help achieving them.

Use an append-only store to record the full series of events that describe actions taken on data in a domain, rather than storing just the current state, so that the store can be used to materialize the domain objects. This pattern can simplify tasks in complex domains by avoiding the requirement to synchronize the data model and the business domain; improve performance, scalability, and responsiveness; provide consistency for transactional data; and maintain full audit trails and history that may enable compensating actions.

image::event-sourcing-exemple.png[Event sourcing Example]

Most applications work with data, and the typical approach is for the application to maintain the current state of the data by updating it as users work with the data. For example, in the traditional create, read, update, and delete (CRUD) model a typical data process will be to read data from the store, make some modifications to it, and update the current state of the data with the new valuesâ€”often by using transactions that lock the data. The `Event Sourcing pattern` defines an approach to handling operations on data that is driven by a sequence of events, each of which is recorded in an append-only store. Application code sends a series of events that imperatively describe each action that has occurred on the data to the event store, where they are persisted. Each event represents a set of changes to the data (such as AddedItemToOrder).



[TIP]
{project-name} will help us to handle replayable events from any kind of source due to its event sourcing core philosophy.

== Data driven architecture

image::data-driven-computing.png[Data Driven]


== Technical design

{project-name} is an event processing framework based on Kafka and Spark. The main goal of this Open Source platform is to
abstract the level of complexity of complex event processing at scale. Of course many people start with an ELK stack,
which is really great but not enough to elaborate a really complete system monitoring tool.
So with {project-name}, you'll move the log processing burden to a powerful distributed stack.

Kafka acts a the distributed message queue middleware while Spark is the core of the distributed processing.
{project-name} glue those technologies to simplify log complex event processing at scale.


image::logisland-workflow.png[]


== Application Architecture

This means that a {project-name} based application will go through a few different distinct phases, each of which can
potentially be executed in a different JVM. These phases are:

Augmentation::
This involves processing all the metadata that is present in the application and its libraries, such as annotations,
descriptors etc, and processing this information to create bytecode that is executable at runtime. This generated bytecode
will directly start the runtime services that were represented by the metadata. For example if your application contains
a `@WebServlet` annotation this phase will output the bytecode required to start Undertow and register this Servlet.
The end result of this phase is a runnable application that should be equivalent to manually wiring up the runtime
services you require, without any of the deployment time code present.

JVM Startup::
When running as a normal JVM application the bytecode generated in the augmentation phase will be executed to start the
runtime services needed by the application. In production mode this will generally be in a different JVM instance, which
means that none of the classes needed by the augmentation phase will be loaded at runtime, resulting in a smaller memory
footprint and faster startup time. In development mode this will be the same JVM, allowing for the application to be
quickly restarted when changes are made.

Native Image Build::
When building a native image with SubstrateVM any code that is part of a static initializer is run as part of the native
image build process, and the results are directly stored in memory. {project-name} takes advantage of this by generating
some of the startup code in static initializer blocks, so they are run as part of the image build process rather than
on image startup. This has multiple advantages, as the image build is a standard JVM it is still possible to use JVM
features such as reflection and dynamic proxy generation that do not work out of the box on Substrate. As most of the
startup is done in this phase the native image will start even faster, as it effectively contains a serialized image of
an already started application.

Native Image Start::
As most of the work has been done in the image build the actual native image startup will generally only contain tasks
that can't be done in advance, such as opening sockets and connecting to databases.

== Extension Architecture

As a result of this architecture every extension will provide two artifacts, one that handles build time processing (the
`-deployment` artifacts), and another `-runtime` artifact that contains the classes needed at runtime. Only the runtime
artifact will end up in the final application. The end result of the deployment process is some generated bytecode that
can directly start any runtime services required by the application (but don't worry, you don't have to know anything
about bytecode to write a {project-name} extension).

{project-name} uses a novel technique for writing bytecode that we are calling _Bytecode Recording_. Basically at augment
time extensions can inject instances of classes that contain the runtime logic to start services, and invoke these
instances as if they were directly starting the service. These injected instances are actually proxies, that override all
the public methods and record exactly which invocations have been made, what the parameters are, and the order they were
made in. They then generate bytecode to perform this exact same sequence of invocations when the application starts up.

This means that from the point of view of an extension developer it looks like they are making invocations that
directly start the application, when in fact they are recording bytecode that will be used to start the application
later.

The `-deployment` artifacts work by defining `@BuildStep` methods, that produce and consume `BuildItem` instances, and
can also record bytecode if required.