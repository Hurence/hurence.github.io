////
This guide is maintained in the main Logisland repository
and pull requests should be submitted there:
https://github.com/hurence/logisland/tree/master/docs/src/main/asciidoc
////

include::./attributes.adoc[]
= Datastores - SolR

:toc: right
:toclevels: 4
:doctype: book
:icons: font
:docinfo1:

:numbered:
:sectnums:
:sectnumlevels: 4



Learn how to send {project-name} `Records` into SolR through de the Datastore API

This guide covers:

* datastore API
* SolR setup

== Prerequisites

To complete this guide, you need:

* less than 15 minutes
* an IDE
* JDK 1.8+ installed with `JAVA_HOME` configured appropriately
* Apache Maven 3.5.3+
* The completed greeter application from the link:getting-started-guide[Getting Started Guide]

== Solution

We recommend that you follow the instructions in the next sections and create the application step by step.
However, you can go right to the completed example.

Clone the Git repository: `git clone {quickstarts-clone-url}`, or download an {quickstarts-archive-url}[archive].

The solution is located in the `datastore-solr` directory.

This guide assumes you already have the completed application from the `getting-started` directory.






// todo 
* add DDC threshold param to record converter
* add processor test
* cleanup structured stream serializer
*  groupby.keys: tagname and group.by.field: tagname to rename
*  spark.sqlContext.setConf("spark.sql.shuffle.partitions", "4") hardcoded
* add watemarking
* test spark.streaming.timeout in StructuredTest