////
This guide is maintained in the main Logisland repository
and pull requests should be submitted there:
https://github.com/hurence/logisland/tree/master/docs/src/main/asciidoc
////

include::./attributes.adoc[]
= {project-name} - Query Matching

:toc: macro
:toclevels: 4
:doctype: book
:icons: font
:docinfo1:

:numbered:
:sectnums:
:sectnumlevels: 4


Learn how to percolate lucen Queries to generate custom events based on business policies
This guide covers:

* lucene syntax
* matching queries

== Prerequisites

To complete this guide, you need:

* less than 15 minutes
* an IDE
* JDK 1.8+ installed with `JAVA_HOME` configured appropriately
* Apache Maven 3.5.3+
* The completed greeter application from the link:event-aggregation-guide[Event Aggregation Guide]


== Architecture

In this guide, we expand on the initial stream that was created as part of the Event Aggregation Guide.
We cover how to raise custom alerts based on lucene matching query criterion.


image::logisland-stream-match-query.png[Archi]

== Solution

We recommend that you follow the instructions in the next sections and create the application step by step.
However, you can go right to the completed example.

* Clone the Git repository: `git clone {quickstarts-clone-url}`, or download an {quickstarts-archive-url}[archive].
* The solution is located in the `query-matching` directory.
* This guide assumes you already have the completed application from the `event-aggregation ` directory.


== Logisland job setup

The logisland job for this tutorial is already packaged in the tar.gz assembly and you can find it here :

Our application will be composed of 4 streams :

The first one converts apache logs to typed records (please note the use of ``ConvertFieldsType`` processor)

The second one is the sql stream is a special one one use a link:../extensions/kafkarecordstreamsqlaggregator.html[KafkaRecordStreamSQLAggregator].
This stream defines input/output topics names as well as Serializers, avro schema.

TIP: The http://avro.apache.org/docs/1.7.7/spec.html[Avro schema] is set for the input topic and must be same as the avro schema of the output topic for the stream that
produces the records (please refer to link:getting-started-guide.html[Getting Started Guide])

The most important part of the `KafkaRecordStreamSQLAggregator` is its `sql.query` property which defines
a query to apply on the incoming records for the given time window.

The following SQL query will be applied on sliding window of 10" of records.

[source,sql,subs=attributes+]
----
SELECT count(*) AS connections_count, 
       avg(bytes_out) AS avg_bytes_out, 
       src_ip, 
       first(record_time) as record_time
FROM logisland_events
GROUP BY src_ip
ORDER BY connections_count DESC
LIMIT 20
----    


which will consider ``logisland_events`` topic as SQL table and create 20 output Record with the fields avg_bytes_out, src_ip & record_time.
the statement with record_time will ensure that the created Records will correspond to the effective input event time (not the actual time).


[source,yaml,subs=attributes+]
----
- stream: metrics_by_host
    component: com.hurence.logisland.stream.spark.KafkaRecordStreamSQLAggregator
    type: stream
    documentation: a processor that links
    configuration:
    kafka.input.topics: logisland_events
    kafka.output.topics: logisland_aggregations
    kafka.error.topics: logisland_errors
    kafka.input.topics.serializer: com.hurence.logisland.serializer.KryoSerializer
    kafka.output.topics.serializer: com.hurence.logisland.serializer.KryoSerializer
    kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
    kafka.metadata.broker.list: sandbox:9092
    kafka.zookeeper.quorum: sandbox:2181
    kafka.topic.autoCreate: true
    kafka.topic.default.partitions: 2
    kafka.topic.default.replicationFactor: 1
    window.duration: 10
    avro.input.schema: >
        {  "version":1,
            "type": "record",
            "name": "com.hurence.logisland.record.apache_log",
            "fields": [
            { "name": "record_errors",   "type": [ {"type": "array", "items": "string"},"null"] },
            { "name": "record_raw_key", "type": ["string","null"] },
            { "name": "record_raw_value", "type": ["string","null"] },
            { "name": "record_id",   "type": ["string"] },
            { "name": "record_time", "type": ["long"] },
            { "name": "record_type", "type": ["string"] },
            { "name": "src_ip",      "type": ["string","null"] },
            { "name": "http_method", "type": ["string","null"] },
            { "name": "bytes_out",   "type": ["long","null"] },
            { "name": "http_query",  "type": ["string","null"] },
            { "name": "http_version","type": ["string","null"] },
            { "name": "http_status", "type": ["string","null"] },
            { "name": "identd",      "type": ["string","null"] },
            { "name": "user",        "type": ["string","null"] }    ]}
    sql.query: >
        SELECT count(*) AS connections_count, avg(bytes_out) AS avg_bytes_out, src_ip
        FROM logisland_events
        GROUP BY src_ip
        ORDER BY event_count DESC
        LIMIT 20
    max.results.count: 1000
    output.record.type: top_client_metrics
----

Here we will compute every x seconds, the top twenty `src_ip` for connections count.
The result of the query will be pushed into to `logisland_aggregations` topic as new `top_client_metrics` Record containing `connections_count` and `avg_bytes_out` fields.


the third match some criteria to send some alerts

[source,yaml,subs=attributes+]
----
- processor: match_query
    component: com.hurence.logisland.processor.MatchQuery
    type: processor
    documentation: a parser that produce alerts from lucene queries
    configuration:
    numeric.fields: bytes_out,connections_count
    too_much_bandwidth: avg_bytes_out:[25000 TO 5000000]
    too_many_connections: connections_count:[150 TO 300]
    output.record.type: threshold_alert
----




== Lucene Query Syntax
Here are some query examples demonstrating the query syntax.

*Keyword matching*

* Search for word "foo" in the title field : `title:foo`
* Search for phrase "foo bar" in the title field : `title:"foo bar"`
* Search for phrase "foo bar" in the title field AND the phrase "quick fox" in the body field : `title:"foo bar" AND body:"quick fox"`
* Search for either the phrase "foo bar" in the title field AND the phrase "quick fox" in the body field, or the word "fox" in the title field : `(title:"foo bar" AND body:"quick fox") OR title:fox`
* Search for word "foo" and not "bar" in the title field : `title:foo -title:bar`

*Wildcard matching*

* Search for any word that starts with "foo" in the title field : `title:foo*`
* Search for any word that starts with "foo" and ends with bar in the title field : `title:foo*bar`

Note that Lucene doesn't support using a * symbol as the first character of a search.

*Proximity matching*

Lucene supports finding words are a within a specific distance away.

* Search for "foo bar" within 4 words from each other : `"foo bar"~4`

Note that for proximity searches, exact matches are proximity zero, and word transpositions (bar foo) are proximity 1.

A query such as `"foo bar"~10000000` is an interesting alternative to `foo AND bar`.

Whilst both queries are effectively equivalent with respect to the documents that are returned, the proximity query assigns a higher score to documents for which the terms foo and bar are closer together.

*Range searches*

Range Queries allow one to match documents whose field(s) values are between the lower and upper bound specified by the Range Query. Range Queries can be inclusive or exclusive of the upper and lower bounds. Sorting is done lexicographically.

    mod_date:[20020101 TO 20030101]

Solr's built-in field types are very convenient for performing range queries on numbers without requiring padding.



== Launch the script
For this tutorial we will handle some apache logs with a splitText parser and send them to Elastiscearch
Connect a shell to your logisland container to launch the following streaming jobs.

    `docker exec -i -t logisland bin/logisland.sh --conf conf/query-matching.yml`


== Check your alerts with Kibana

As we explore data logs from july 1995 we'll have to select an absolute time filter from 1995-06-30 to 1995-07-08 to see the events.

image::kibana-logisland-aggregates-events.png[alt=Kibana]

you can filter your events with `record_type:connection_alert` to get 71733 connections alerts matching your query

image::kibana-blacklisted-host.png[alt=Kibana]

if we filter now on threshold alerts whith `record_type:threshold_alert` you'll get the 13 src_ip that have been catched by the threshold query.

image::kibana-threshold-alerts.png[alt=Kibana]
